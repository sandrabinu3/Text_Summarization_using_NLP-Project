{"cells":[{"cell_type":"markdown","metadata":{"id":"pQdDmZNC-MTp"},"source":["# Google Drive and GitHub"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3708,"status":"ok","timestamp":1734687211291,"user":{"displayName":"Sandra Binu","userId":"06782843066512118455"},"user_tz":0},"id":"QewiaJ_QIaDy","outputId":"614b7b7a-7cb3-470e-eeb2-05da5b8d098d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","# Mount Google Drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1734687211291,"user":{"displayName":"Sandra Binu","userId":"06782843066512118455"},"user_tz":0},"id":"_g5X0pUbvUut","outputId":"d80eb3cd-2d51-473b-b9b4-a5900113ce26"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Project/Text_Summarization_using_NLP-Project\n"]}],"source":["%cd /content/drive/MyDrive/Project/Text_Summarization_using_NLP-Project"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1734687211292,"user":{"displayName":"Sandra Binu","userId":"06782843066512118455"},"user_tz":0},"id":"xDcaJfN2vjCU","outputId":"23b2705a-0d2a-4ccc-c464-d4cacf6b00f4"},"outputs":[{"output_type":"stream","name":"stdout","text":[" lit.docx  'Project Data Management Plan.docx'\t Reference   textSum.ipynb\n"," logs\t    Project_NLP.ipynb\t\t\t results     wandb\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsFGgGUIjegr","outputId":"05c47765-7545-4105-ae5a-851f5eb909de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Refresh index: 100% (40/40), done.\n","On branch main\n"]}],"source":["# Check the status of the repository\n","!git status"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hcA_ALlrjUJK"},"outputs":[],"source":["# Stage the changes\n","# !git add textrank.ipynb  # or use !git add . to stage all changes\n","!git add .\n","\n","!git config --global user.email \"sandrabinu99@gmail.com\"\n","!git config --global user.name \"sandrabinu3\"\n","\n","# # Commit the changes with a message\n","!git commit -m \"evaluation\"\n","\n","# # Push the changes to your GitHub repository\n","# !git push origin main\n","!git push https://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@github.com/sandrabinu3/Text_Summarization_using_NLP-Project.git\n"]},{"cell_type":"markdown","metadata":{"id":"me9ozi-H-b5L"},"source":["# Libraries\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1RGuWVjrnavm"},"outputs":[],"source":["!pip install rouge\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jGIQgMucIL74"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from wordcloud import WordCloud\n","import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import networkx as nx\n","from sklearn.metrics.pairwise import cosine_similarity\n","from rouge import Rouge\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec # Import the Word2Vec class from the gensim library\n","from tqdm import tqdm\n","from gensim.models import KeyedVectors\n","from datasets import Dataset,load_dataset\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceXXGlEcKEdm"},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTAMe1T_Iege"},"outputs":[],"source":["# Load the CSV files\n","test_df = pd.read_csv('/content/drive/MyDrive/Project/test_data.csv')\n","train_df = pd.read_csv('/content/drive/MyDrive/Project/train_data.csv')\n","validation_df = pd.read_csv('/content/drive/MyDrive/Project/validation_data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WURH-hv0GCk_"},"outputs":[],"source":["train_subset = train_df.sample(n=10000,random_state=33)\n","test_subset = test_df.sample(n=2000,random_state=33)\n","val_subset = validation_df.sample(n=100,random_state=20)"]},{"cell_type":"markdown","metadata":{"id":"O72cr34CAWTM"},"source":["# Exploratory Data Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nvSTvrTrcmZF"},"outputs":[],"source":["# Check the shape of each dataset\n","print(f\"Train set: {train_df.shape}\")\n","print(f\"Test set: {test_df.shape}\")\n","print(f\"Validation set: {validation_df.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eedSm9c-cpfc"},"outputs":[],"source":["#check for the basic informations and null values\n","print(train_subset.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0q8brDycrgX"},"outputs":[],"source":["print(test_subset.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UN2szRUzcu_W"},"outputs":[],"source":["print(val_subset.info())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8-f8AbTxdNEW"},"outputs":[],"source":["\n","print(train_subset.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWZ2Kbk4_5u5"},"outputs":[],"source":["# length of words in article and summary\n","train_subset['article_length'] = train_subset['article'].apply(lambda x: len(x.split()))\n","train_subset['summary_length'] = train_subset['highlights'].apply(lambda x: len(x.split()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY9o1M5JdRl6"},"outputs":[],"source":["# Visualization\n","sns.histplot(train_subset['article_length'], bins=50, kde=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94PD9tP7dWDA"},"outputs":[],"source":["sns.histplot(train_subset['summary_length'], bins=50, kde=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xk8dWaFEmahp"},"outputs":[],"source":["# Summary statistics\n","summary_stats = train_subset['summary_length'].describe()\n","print(summary_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EehNBuaXdhF6"},"outputs":[],"source":["### If the standard deviation is large (e.g., comparable to the mean) or\n","###  the histogram shows a wide range of lengths, there may be inconsistency in the summary lengths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YDdKdLd-J_sB"},"outputs":[],"source":["stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gueGfNS1dv-I"},"outputs":[],"source":["wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\").generate(\" \".join(train_subset['article']))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5_FwFmaEslk3"},"outputs":[],"source":["val_subset.head()"]},{"cell_type":"markdown","metadata":{"id":"RrA5lHtNAnlv"},"source":["# TextRank"]},{"cell_type":"markdown","metadata":{"id":"pQqhyNZlAxPs"},"source":["Data Preprocessing for TextRank"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSGZJbMdJPzk"},"outputs":[],"source":["val_summaries = val_subset['highlights'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEr29VIkGLTv"},"outputs":[],"source":["##Function to preprocess the text\n","def preprocess_text_tr(text):\n","    # Convert text to lowercase\n","    text = text.lower()\n","\n","    # Remove special characters, numbers, and extra spaces\n","    text = re.sub(r'\\W',' ',text)\n","\n","    # tokenize sentences and words\n","    words = word_tokenize(text)\n","\n","    # Remove stopwords and perform lemmatization\n","    processed_words = [lemmatizer.lemmatize(word) for word in words\n","                       if word not in stop_words]\n","\n","    # Return cleaned text as a single string\n","    return ' '.join(processed_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yJaU5EvPG2st"},"outputs":[],"source":["# Function to split text into sentences\n","def split_into_sentences(text):\n","    return sent_tokenize(text)"]},{"cell_type":"markdown","metadata":{"id":"JB2cFaQzA79G"},"source":["TextRank Model Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RuhMc_U4G5kw"},"outputs":[],"source":["\n","# TextRank Summarization Function\n","def textrank_summarizer(text, top_n=3):\n","    # Preprocess and split text into sentences\n","    sentences = split_into_sentences(text)\n","\n","    # If there is only one sentence, return it as the summary\n","    if len(sentences) <= 1:\n","        return text\n","    # Preprocess each sentence\n","    cleaned_sentences = [preprocess_text_tr(sentence) for sentence in sentences]\n","\n","    # Vectorize the sentences using TF-IDF\n","    tfidf_vectorizer = TfidfVectorizer()\n","    tfidf_matrix = tfidf_vectorizer.fit_transform(cleaned_sentences)\n","\n","    # Compute cosine similarity matrix\n","    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n","\n","   # graph is created from similarity matrix\n","    nx_graph = nx.from_numpy_array(similarity_matrix)\n","\n","    #calculate sentence scores using PageRank algorithm\n","    scores = nx.pagerank(nx_graph)\n","\n","    # Rank the sentences based on their scores\n","    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n","\n","    # Select the top N sentences for the summary\n","    summary = \" \".join([ranked_sentences[i][1] for i in range(min(top_n, len(ranked_sentences)))])\n","\n","    return summary"]},{"cell_type":"markdown","metadata":{"id":"zwgusER3BRwK"},"source":["\n","TextRank Evaluation\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T993qOImSdh7"},"outputs":[],"source":["# Run summarization on validation set for initial evaluation\n","textrank_val_summaries = [textrank_summarizer(article) for article\n","                          in val_subset['article']]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BC1WkhAWSdVI"},"outputs":[],"source":["rouge = Rouge()\n","scores = rouge.get_scores(textrank_val_summaries,val_summaries,avg=True)\n","print(\"TextRank ROUGE Scores:\")\n","scores"]},{"cell_type":"markdown","metadata":{"id":"aClsDL1qBdnS"},"source":["# Word2Vec"]},{"cell_type":"markdown","metadata":{"id":"yLkFj_ssFF3h"},"source":["Data Preprocessing for Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CwiMFHXGwW9"},"outputs":[],"source":["def preprocess_text_wv(text):\n","    # Basic text cleaning\n","    text=re.sub(r'\\W',' ', text)\n","\n","    # Sentence tokenization\n","    sentences = sent_tokenize(text)\n","    processed_sentences = []\n","\n","    # Word tokenization and stopword removal\n","    for sentence in sentences:\n","        words = word_tokenize(sentence.lower())\n","        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n","        processed_sentences.append(words)\n","\n","    return sentences, processed_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VVfj0kbLGwTk"},"outputs":[],"source":["processed_w2v = [preprocess_text_wv(article) for article in tqdm(train_subset['article'])]\n","tokenized_sentences = [tokens for _, tokenized in processed_w2v for tokens in tokenized]"]},{"cell_type":"markdown","metadata":{"id":"1tty5Do6BzDx"},"source":["Word2Vec Training and Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HVkeWI4bGwQp"},"outputs":[],"source":["w_model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=5,min_count=5,workers=5, sg=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAv6Ja1BGwN5"},"outputs":[],"source":["def sentence_to_vector(sentence, model):\n","    #Convert a sentence to a vector by averaging word vectors.\n","\n","    words = preprocess_text_wv(sentence)\n","    # Flatten the list if `preprocess_text` returns a list of lists\n","    if isinstance(words[0], list): # check if words is list of lists\n","        words = [word for sublist in words for word in sublist] # flatten it\n","\n","    word_vectors = []\n","\n","    # Get word vectors for words in the sentence\n","    for word in words:\n","        # Convert the word to string before checking if it's in the vocabulary\n","        if isinstance(word, list): # Check if word is a list\n","            word = ' '.join(word) # Convert the list to string\n","\n","        if word in model.wv.key_to_index:  # Check if the word is in the model's vocabulary\n","            word_vectors.append(model.wv[word])\n","\n","    # Return the average word vector for the sentence\n","    if word_vectors:\n","        return np.mean(word_vectors, axis=0)\n","    else:\n","        # If no word vectors found, return a zero vector (for empty sentences or unknown words)\n","        return np.zeros(model.vector_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwscH7FyGwK1"},"outputs":[],"source":["def calculate_sentence_similarity(sentences, model):\n","  #Calculate the cosine similarity between each pair of sentences based on their vectors.\n","\n","    # Convert each sentence to its vector representation\n","    sentence_vectors = np.array([sentence_to_vector(sentence, model) for sentence in sentences])\n","\n","    # Compute the cosine similarity matrix\n","    similarity_matrix = cosine_similarity(sentence_vectors)\n","    return similarity_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdUTmTmWGwH-"},"outputs":[],"source":["def extractive_summary(text, model, top_n=3):\n","    #Generate an extractive summary by selecting top n sentences based on cosine similarity.\n","\n","    sentences = text.split('.')  # Split text into sentences\n","    similarity_matrix = calculate_sentence_similarity(sentences, model)\n","\n","    # Get similarity scores for each sentence (using the first sentence as a reference)\n","    similarity_scores = similarity_matrix[0]  # Assuming first sentence as the reference\n","\n","    # Rank sentences by similarity scores\n","    sorted_similarities = sorted(enumerate(similarity_scores), key=lambda x: x[1], reverse=True)\n","\n","    # Extract top N sentences\n","    top_sentences = [sentences[idx] for idx, _ in sorted_similarities[:top_n]]\n","    return ' '.join(top_sentences)\n"]},{"cell_type":"markdown","metadata":{"id":"y9IxDRnCB9L9"},"source":["Word2Vec Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UDSLl-ZOGwFf"},"outputs":[],"source":["# For validation data\n","validation_summaries_wv = []\n","for idx, row in val_subset.iterrows():\n","    text = row['article']\n","    summary = extractive_summary(text, w_model, top_n=3)  # Extract top 3 sentences for the summary\n","    validation_summaries_wv.append(summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA6b2TXYGwCg"},"outputs":[],"source":["# Add the summary to the validation DataFrame\n","val_subset['summary_wv'] = validation_summaries_wv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQCkwgTtGv_u"},"outputs":[],"source":["val_summaries = val_subset['highlights'].tolist()\n","w2v_val_summaries = val_subset['summary_wv'].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWBz9E-EGv9N"},"outputs":[],"source":["rouge = Rouge()\n","w2v_scores = rouge.get_scores(w2v_val_summaries, val_summaries,avg=True)\n","w2v_scores"]},{"cell_type":"markdown","metadata":{"id":"7sNGCTLYCCrF"},"source":["# T5"]},{"cell_type":"markdown","metadata":{"id":"QjZjU0XlPoiH"},"source":["Data Preprocessing for T5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ieNrqBNPoDT"},"outputs":[],"source":["# Convert pandas DataFrame to Hugging Face Dataset\n","train_dataset = Dataset.from_pandas(train_subset)\n","test_dataset = Dataset.from_pandas(test_subset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fR2XMxlsPoAq"},"outputs":[],"source":["def preprocess_data_t5(data, tokenizer, max_input_length=512, max_target_length=128):\n","    # Add the 'summarize:' prefix to each article to indicate the task type\n","    inputs = [\"summarize: \" + doc for doc in data[\"article\"]]\n","    # Get the target summaries (highlights)\n","    targets = data['highlights']\n","    # Tokenize the input articles\n","    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n","    # Tokenize the target summaries (highlights)\n","    labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n","    # Assign the tokenized target summaries as labels for the model\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    # Return the processed inputs and labels in the required format\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FAs3HbPPoGn"},"outputs":[],"source":["# Load tokenizer and model\n","model_name = \"t5-small\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LKUWPZjFPn7B"},"outputs":[],"source":["# preprocess and tokenize datasets\n","train_dataset = train_dataset.map(lambda x: preprocess_data_t5(x, tokenizer), batched=True)\n","test_dataset = test_dataset.map(lambda x: preprocess_data_t5(x, tokenizer), batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i_z8eAUGPn4Z"},"outputs":[],"source":["# Define the list of columns to be included in the PyTorch format\n","columns = ['input_ids', 'attention_mask', 'labels']\n","\n","# Convert the train dataset into a PyTorch-compatible format\n","# by setting the specified columns as torch tensors\n","train_dataset.set_format(type=\"torch\", columns=columns)\n","\n","# Convert the test dataset into a PyTorch-compatible format\n","# by setting the specified columns as torch tensors\n","test_dataset.set_format(type=\"torch\", columns=columns)"]},{"cell_type":"markdown","metadata":{"id":"WJaXxS9uCNv6"},"source":["T5 Initialization and Fine-Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NJCwSO2Pnyk"},"outputs":[],"source":["# Define data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer=tokenizer,\n","    model=model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"phH4OhcXPn1h"},"outputs":[],"source":["# Define the training arguments for fine-tuning the model\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=\"/content/drive/MyDrive/Project/t5-small-summarizer\",  # Directory to save model checkpoints and results\n","    eval_strategy=\"epoch\",  # Evaluation strategy: perform evaluation at the end of each training epoch\n","    learning_rate=2e-5,  # Learning rate for the optimizer (how much to adjust the weights during training)\n","    per_device_train_batch_size=18,  # Batch size for training on each device (GPU/CPU)\n","    per_device_eval_batch_size=18,  # Batch size for evaluation on each device\n","    num_train_epochs=3,  # Number of times to go through the entire training dataset\n","    save_steps=500,  # Number of steps between model checkpoint saves\n","    save_total_limit=2,  # Limit the number of saved checkpoints to avoid excessive storage usage\n","    predict_with_generate=True,  # Whether to use the model’s text generation functionality for predictions (for Seq2Seq tasks like summarization)\n","    fp16=True,  # Whether to use mixed precision training (use half-precision floating point format) to reduce memory usage and speed up training\n","    weight_decay=0.01,  # Weight decay (L2 regularization) to prevent overfitting\n","    metric_for_best_model=\"loss\")  # The metric used to determine the best model (based on the lowest loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STzMHUTcPnvh"},"outputs":[],"source":["# Initialize Trainer\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_BkJJbG0n-z0"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"fM0a6cNbCUQs"},"source":["T5 Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DOH13ryn-wd"},"outputs":[],"source":["model_name='/content/drive/MyDrive/Project/t5-small-summarizer/checkpoint-5000'\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbRmjXxMn-tb"},"outputs":[],"source":["# Initialize a list to store generated summaries and highlights\n","generated_summaries = []\n","\n","# Iterate through each article in the validation dataset\n","for i in range(len(val_subset)):\n","    val_articles = val_subset.iloc[i]['article']\n","    highlight = val_subset.iloc[i]['highlights']\n","\n","    # Tokenize the input text\n","    inputs = tokenizer(val_articles, max_length=512, truncation=True, return_tensors='pt')\n","\n","    # Generate the summary\n","    summary_ids = model.generate(\n","        **inputs,\n","        max_length=150,\n","        num_beams=5,\n","        do_sample=True,\n","        temperature=1.2,  # Encourages more randomness\n","        top_k=100,         # Considers top 50 probable tokens\n","        top_p=0.95,        # Samples from the top 90% probability mass\n","        repetition_penalty=1.1,  # Penalizes token repetition\n","        early_stopping=True)\n","\n","    # Decode the summary and append it to the list\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    generated_summaries.append(summary)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMgGKFF0n-pr"},"outputs":[],"source":["val_summaries = val_subset['highlights'].tolist()\n","gen_summaries = generated_summaries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Frh58MW3n-mz"},"outputs":[],"source":["rouge = Rouge()\n","t5_scores=rouge.get_scores(gen_summaries,val_summaries,avg=True)\n","t5_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aO6IFd42BxR9"},"outputs":[],"source":["gen_summaries[9]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhfFXM1ZB0wB"},"outputs":[],"source":["val_summaries[9]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R31xywAxikxj"},"outputs":[],"source":["val_subset.iloc[9]['article']"]},{"cell_type":"markdown","metadata":{"id":"yzReOvKA58xL"},"source":["# **Comparison and Anlaysis**\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y2oY8UHeCFBA"},"outputs":[],"source":["# Combine all scores into a single dictionary\n","rouge_scores = {\n","    'TextRank': scores,\n","    'Word2Vec': w2v_scores,\n","    'T5': t5_scores\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpdfo4aB5x2P"},"outputs":[],"source":["# Print the combined dictionary\n","print(rouge_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MT8hPSG8vlCC"},"outputs":[],"source":["# Convert to DataFrame\n","df = pd.DataFrame.from_dict({(model, metric): values\n","                             for model, metrics in rouge_scores.items()\n","                             for metric, values in metrics.items()})\n","\n","df = df.transpose()\n","df.reset_index(inplace=True)\n","df.columns = ['Model', 'Metric', 'Recall', 'Precision', 'F1-Score']\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"of1YLg4h5xyu"},"outputs":[],"source":["# Flattening the data\n","rows = []\n","for models, metrics in rouge_scores.items():\n","    for metric, scores in metrics.items():\n","        for score_type, value in scores.items():\n","            rows.append({\n","                'Model': models,\n","                'Metric': metric,\n","                'Score Type': score_type,\n","                'Value': value\n","            })\n","\n","# Creating the DataFrame\n","df = pd.DataFrame(rows)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBr2sfWB5xwL"},"outputs":[],"source":["plt.figure(figsize=(8, 6))\n","sns.barplot(data=df, x='Metric', y='Value', hue='Model', errorbar=None, palette='viridis')\n","plt.title('ROUGE Scores by Metric and Model', fontsize=14)\n","plt.ylabel('Score', fontsize=12)\n","plt.xlabel('ROUGE Metric', fontsize=12)\n","plt.legend(title='Model')\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LZxePT9K5xlj"},"outputs":[],"source":["# Separate the data by Metric and plot each as a bar plot\n","metrics = ['rouge-1', 'rouge-2', 'rouge-l']\n","for metric in metrics:\n","    plt.figure(figsize=(8, 6))\n","    sns.barplot(\n","        data=df[df['Metric'] == metric],\n","        x='Score Type', y='Value', hue='Model', palette='Set2'\n","    )\n","    plt.title(f'Comparison of {metric.upper()} Scores by Model', fontsize=16)\n","    plt.ylabel('Score', fontsize=12)\n","    plt.xlabel('Score Type (Recall, Precision, F1)', fontsize=12)\n","    plt.legend(title='Model', loc='upper center')\n","    plt.grid(axis='y', linestyle='--', alpha=0.7)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2zafMVB9Uhp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyM5wMLz37TwttNJWNfPX/nC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}